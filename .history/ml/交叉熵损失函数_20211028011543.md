# 3. 极大似然估计
&emsp;&emsp;假设我们需要调查我们学校学生的身高分布。我们先假设学校所有学生的身高服从正态分布 $N(\mu, \sigma^2)$ 。(**注意：极大似然估计的前提一定是要假设数据总体的分布，如果不知道数据分布，是无法使用极大似然估计的**)，这个分布的均值 $\mu$ 和方差 $\sigma^2$ 未知，如果我们估计出这两个参数，那我们就得到了最终的结果。那怎样估计这两个参数呢？


&emsp;&emsp;学校的学生这么多，我们不可能挨个统计。这时候我们需要用到概率统计的思想，也就是抽样，根据样本估算总体。假设我们随机抽到了 200 个人（也就是 200 个身高的样本数据）。然后统计抽样这 200 个人的身高。根据这 200 个人的身高估计均值 $\mu$ 和方差 $\sigma^2$ 。

&emsp;&emsp;用数学的语言来说就是：为了统计学校学生的身高分布，我们独立地按照概率密度 $p(x|\theta)$ 抽取了 200 个样本，组成样本集 $X=x_1,x_2,...,x_N$ (其中 $x_i$ 表示抽到的第 $i$ 个人的身高，这里 $N$ 就是 200，表示样本个数)，我们想通过样本集 $X$ 来估计出总体的未知参数 $\theta$ 。这里概率密度 $p(x|\theta)$ 服从高斯分布 $N(\mu, \sigma^2)$ ，其中的未知参数是 $\theta=[\mu,\sigma^2]$ 。

&emsp;&emsp;那么问题来了怎样估算参数 $\theta$ 呢

## 3.1 参数估计
首先看几个小问题：

**问题1：抽到这200个人的概率是多少？**

&emsp;&emsp;由于每个样本都是独立地从 $p(x|\theta)$ 中抽取的，换句话说这 200 个学生随便捉的，他们之间是没有关系的，即他们之间是相互独立的。假如抽到学生 A（的身高）的概率是 $p(x_A|\theta)$ ，抽到学生B的概率是 $p(x_B|\theta)$ ，那么同时抽到男生 A 和男生 B 的概率是 $p(x_A|\theta)\times p(x_B|\theta)$ ，同理，我同时抽到这 200 个学生的概率就是他们各自概率的乘积了，即为他们的联合概率，用下式表示：
$$L(\theta)=L(x_1,x_2,...,x_n;\theta)=\prod_{i=1}^{n}p(x_i|\theta),\quad \theta\in\Theta$$
n 为抽取的样本的个数，本例中 $n=200$ ，这个概率反映了，在概率密度函数的参数是 $\theta$ 时，得到 $X$ 这组样本的概率。上式中等式右侧只有 $\theta$ 是未知数，所以 $L$ 是 $\theta$ 的函数。

&emsp;&emsp;这个函数反映的是在不同的参数 $\theta$ 取值下，取得当前这个样本集的可能性，因此称为参数 $\theta$ 相对于样本集 $X$ 的似然函数（likelihood function），记为 $L(\theta)$ 。

&emsp;&emsp;对 L 取对数，将其变成连加的，称为对数似然函数，如下式：
$$H(\theta) = \ln L(\theta)=\ln \prod_{i=1}^{n}p(x_i|\theta)=\underset{i=1}{\overset{n}{\Sigma}}\ln p(x_i|\theta)$$

**Q：为什么这里取对数？**

* 取对数之后累积变为类和，求导更方便
* 概率累积会出现数值非常小的情况，比如$1e-30$，由于计算机的精度是有限的，无法识别这一类数，取对数之后，更易于计算机识别

**问题2：学校那么多学生，为什么就恰好抽到了这200个人呢？**

&emsp;&emsp;在学校那么多人中，我一抽就抽到这 200 个人，而不是其他人，那是不是表示在整个学校中，这 200 个人出现的概率极大啊，也就是其对应的似然函数 $L(\theta)$ 极大，即

$$\hat{\theta}=\argmax L(\theta)$$

$\hat{\theta}$ 这个叫做 $\theta$ 的极大似然估计量，即为我们所求的值。

**问题3：怎么求解极大似然函数？**

&emsp;&emsp;求 $L(\theta)$ 对所有参数的偏导数，然后让这些偏导数为 0，假设有 $n$ 个参数，就有 $n$ 个方程组成的方程组，那么方程组的解就是似然函数的极值点了，从而得到对应的 $\theta$ 了。
## 3.2 极大似然估计总结
&emsp;&emsp; 极大似然估计你可以看作是一个反推。很多时候我们是根据已知参数来推算结果，而极大似然估计是已经知道结果，然后寻求使该结果出现的可能性极大的参数，以此作为参数的估计值。

&emsp;&emsp; 举个例子：假如一个学校的学生男女比例为 9:1 (条件)，那么你可以推出，你在这个学校里更大可能性遇到的是男生 (结果)；假如你不知道那女比例，你走在路上，碰到100个人，发现男生就有90个 (结果)，这时候你可以推断这个学校的男女比例更有可能为 9:1 (条件)，这就是极大似然估计。

&emsp;&emsp; 极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，通过若干次试验，观察其结果，利用结果推出参数的估计值。
## 3.3 极大似然估计的一般步骤
&emsp;&emsp; (1). 写出似然函数；

&emsp;&emsp; (2). 对似然函数取对数，并整理；

&emsp;&emsp; (3). 求导数，令导数为 0，得到似然方程；

&emsp;&emsp; (4). 解似然方程，得到的参数。

## 3.4 极大似然函数的应用
### 应用一 ：回归问题中的最小化平方和 （极小化损失函数）

&emsp;&emsp;假设线性回归模型具有如下形式:
$$h(x)=\underset{i=1}{\overset{d}{\Sigma}}\theta_j x_j+\epsilon=\theta^Tx+\epsilon$$
其中 $x\in R^{1\times d}$ ， 误差 $\epsilon\in R$ ， 如何求 $\theta$ 呢？

1. 最小二乘估计：最合理的参数估计量应该使得模型能最好地拟合样本数据，也就是估计值和观测值之差的平方和最小，其推导过程如下所示：

    $$J(\theta)=\underset{i=1}{\overset{n}{\Sigma}}(h_{\theta}(x_i)-y_i)^2$$
    求解方法是梯度下降算法，不断迭代得到最终值。

2. 极大似然估计法：最合理的参数估计量应该使得从模型中抽取 $n$ 组样本观测值的概率极大，也就是似然函数极大。假设误差项 $\epsilon\in N(0, \sigma^2)$ ，则 $y_i\in N(\theta x_i,\sigma^2)$ (这里涉及到正态分布的概率密度函数和相关的性质)
    $$p(y_i|x_i;\theta)=\frac{1}{\sqrt{2\pi}\sigma}\exp (-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})$$
    $$\begin{aligned}
        L(\theta) & =\prod_{i=1}^{n}p(y_i|x_i;\theta)\\
                  & =\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}\exp (-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})
    \end{aligned}$$
    $$\begin{aligned}
        H(\theta) & =\log (L(\theta))\\
                  & = \log\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}\exp (-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})\\
                  & = \overset{n}{\underset{i=1}{\Sigma}}(\log\frac{1}{\sqrt{2\pi}\sigma}\exp (-\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2}))\\
                  & = -\frac{1}{2\sigma^2}\overset{n}{\underset{i=1}{\Sigma}}(y_i-\theta^Tx_i)^{2}-n\ln \sigma \sqrt{2\pi}
    \end{aligned}$$

    令 $J(\theta)=\frac{1}{2}\overset{n}{\underset{i=1}{\Sigma}}(y_i-\theta^Tx_i)^2$ 则 $\underset{\theta}{\argmax}H(\theta)\Longleftrightarrow\underset{\theta}{\argmin}J(\theta)$ ， 即极大似然函数等价于极小化损失函数。

&emsp;&emsp; `可以发现，此时极大化似然函数和最初的最小二乘损失函数的估计结果是等价的。但是要注意这两者只是恰好有着相同的表达结果，原理和出发点完全不同。`


### 应用二：分类问题中的极小化交叉熵（极小化代价函数）
&emsp;&emsp;在分类问题中，交叉熵的本质就是似然函数的极大化，逻辑回归的假设函数为：
$$h(x)=\hat{y}=\frac{1}{1+e^{-\theta^Tx+b}}$$

&emsp;&emsp;又因为 $\hat{y}=p(y=1|x,\theta)$，所以当 $y=1$ 时，$p_1=p(y=1|x,\theta)=\hat{y}$；当 $y=0$ 时，$p_0=p(y=0|x,\theta)=1-\hat{y}$，合并这两个式子可以得到：
$$p(y|x,\theta) = \hat{y}^y(1-\hat{y})^{1-y}$$
$$\begin{aligned}
    L(\theta)& =\prod_{i=1}^{n}p(y_i|x_i;\theta)\\
             & =\prod_{i=1}^{n}\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}
\end{aligned}$$
$$\begin{aligned}
    H(\theta)& =\log (L(\theta))\\
             & =\log\prod_{i=1}^{n}\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}\\
             & =\overset{n}{\underset{i=1}{\Sigma}}\log \hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}\\
             & = \overset{n}{\underset{i=1}{\Sigma}}{y_i}\log \hat{y_i}+(1-y_i)\log(1-\hat{y_i})
\end{aligned}$$
&emsp;&emsp; 令$J(\theta)=-H(\theta)=-\overset{n}{\underset{i=1}{\Sigma}}{y_i}\log \hat{y_i}+(1-y_i)\log(1-\hat{y_i})$，则$\underset{\theta}{\argmax}H(\theta)\Longleftrightarrow\underset{\theta}{\argmin}J(\theta)$